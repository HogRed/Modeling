{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying MNIST Digits with a Neural Network Made From Scratch\n",
        "\n",
        "This notebook demonstrates how to build and train a simple neural network from scratch. The overall process involves initializing network parameters, performing a forward pass to obtain predictions, computing the loss, and using backpropagation to update the parameters. Below is an overview of the main components.\n",
        "\n",
        "This notebook is heavily inspired by [Samson Zhang](https://www.youtube.com/watch?v=w8yWXqWQYmU)."
      ],
      "metadata": {
        "id": "-HJcBDE3Io9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Architecture\n",
        "\n",
        "We construct our network with an input layer, one or more hidden layers, and an output layer. For each layer \\(l\\), the weighted sum (also known as the pre-activation output) is computed by:\n",
        "\n",
        "$$\n",
        "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
        "$$\n",
        "\n",
        "- **$\\(W^{(l)}\\$)**: Weight matrix for layer \\(l\\).\n",
        "\n",
        "- **$\\(a^{(l-1)}\\$)**: Activation from the previous layer.\n",
        "\n",
        "- **$\\(b^{(l)}\\$)**: Bias term for layer \\(l\\).\n",
        "\n",
        "Here, $(z^{(l)}$) is then passed through an activation function to produce the activation $(a^{(l)}$) for that layer.\n"
      ],
      "metadata": {
        "id": "K7fdqz8-Je3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearities into the network, which enable it to capture complex relationships. Some common choices are:\n",
        "\n",
        "### Sigmoid\n",
        "The sigmoid function maps input values to a range between 0 and 1. Its formula is:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "This function is especially useful in the output layer of binary classifiers.\n",
        "\n",
        "### Rectified Linear Unit (ReLU)\n",
        "ReLU is defined as:\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(z) = \\max(0, z)\n",
        "$$\n",
        "\n",
        "It is popular because it mitigates the vanishing gradient problem during training."
      ],
      "metadata": {
        "id": "QrWnZ3HRJiyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loss Function Explanation\n",
        "\n",
        "While this notebook does not explicitly compute a scalar loss value, it uses the gradient of the loss to update parameters. The key idea is that, when combining softmax with the cross-entropy loss, the derivative with respect to the pre-softmax activations $( Z_2 $) simplifies to:\n",
        "\n",
        "$$\n",
        "dZ_2 = A_2 - Y_{\\text{one-hot}}\n",
        "$$\n",
        "\n",
        "Here, $( Y_{\\text{one-hot}} $) is the one-hot encoded vector of the true label. The corresponding cross-entropy loss is defined as:\n",
        "\n",
        "$$\n",
        "L(Y, A_2) = -\\sum_{i} Y_i \\log(A_{2,i})\n",
        "$$\n",
        "\n",
        "Because of the properties of softmax, its derivative together with cross-entropy yields the simple error term $( A_2 - Y_{\\text{one-hot}} $)."
      ],
      "metadata": {
        "id": "95yreGRZJsXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Backpropagation and Parameter Updates\n",
        "\n",
        "Backpropagation is the key algorithm for training neural networks. It involves computing gradients of the loss function with respect to each parameter. These gradients are then used to update the parameters via gradient descent. The generic update rule is:\n",
        "\n",
        "$$\n",
        "\\theta := \\theta - \\alpha \\frac{\\partial L}{\\partial \\theta}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\( \\theta \\$) represents any network parameter (e.g., \\(W\\) or \\(b\\)).\n",
        "\n",
        "- $\\( \\alpha \\$) is the learning rate, controlling the size of the update step.\n",
        "\n",
        "- $\\( \\frac{\\partial L}{\\partial \\theta} \\$) is the gradient of the loss with respect to the parameter.\n",
        "\n",
        "Gradient computation is carried out using the chain rule.\n",
        "\n",
        "For a weight matrix $W^{(l)}$ in layer $l$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial z^{(l)}} \\cdot \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}\n",
        "$$\n",
        "\n",
        "This derivative tells us how changes in $\\(W^{(l)}\\$) affect the loss, allowing the algorithm to minimize the loss function iteratively."
      ],
      "metadata": {
        "id": "3_v24wPkKwJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Setup"
      ],
      "metadata": {
        "id": "Y23I4Z9tkc2C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "CMlRiYFckPQm"
      },
      "outputs": [],
      "source": [
        "# Package imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### MNIST\n",
        "data = pd.read_csv('/content/mnist_train.csv')"
      ],
      "metadata": {
        "id": "6zfifMbrkUpA"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Formatting"
      ],
      "metadata": {
        "id": "Nd2CjVD2kgG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array(data)\n",
        "m, n = data.shape\n",
        "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
        "\n",
        "data_dev = data[0:1000].T\n",
        "Y_dev = data_dev[0]\n",
        "X_dev = data_dev[1:n]\n",
        "X_dev = X_dev / 255.\n",
        "\n",
        "data_train = data[1000:m].T\n",
        "Y_train = data_train[0]\n",
        "X_train = data_train[1:n]\n",
        "X_train = X_train / 255.\n",
        "_,m_train = X_train.shape"
      ],
      "metadata": {
        "id": "QlqeVsx6kiQ1"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGWdGmuMp4TW",
        "outputId": "c1297b3f-3d97-42b3-83a4-9662d8abc829"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59000,)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msySzh7UsdJS",
        "outputId": "02eccd0e-2d25-4e25-b9f4-873d96b0239a"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 59000)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Net Functions"
      ],
      "metadata": {
        "id": "iMRbVL1qliLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params():\n",
        "    '''\n",
        "    Initializes the weights and biases matrices for the neural network.\n",
        "\n",
        "    np.random.rand() by default provides [0, 1). However, here we subtract by\n",
        "    0.5 to make that [-0.5, 0.5).\n",
        "\n",
        "    Returns initializes weights & biases.\n",
        "    '''\n",
        "    W1 = np.random.rand(10, 784) - 0.5\n",
        "    b1 = np.random.rand(10, 1) - 0.5\n",
        "    W2 = np.random.rand(10, 10) - 0.5\n",
        "    b2 = np.random.rand(10, 1) - 0.5\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def ReLU(Z):\n",
        "    '''\n",
        "    Calculates the ReLU output for a value.\n",
        "    0 if <= 0.\n",
        "    Z if > 0.\n",
        "    '''\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def softmax(Z):\n",
        "    '''\n",
        "    Calculates the softmax function on a group of logits. The resulting output\n",
        "    is a vector of probabilities, the highest of which will be the label that\n",
        "    the model predicts.\n",
        "    '''\n",
        "\n",
        "    # Subtract the max for numerical stability\n",
        "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)\n",
        "\n",
        "    # Calculate probabilities. Here we look at each part with respect to the\n",
        "    # whole, which provides us a ratio (probability)\n",
        "    expZ = np.exp(Z_shifted)\n",
        "\n",
        "    # We add an eps here for numerical stability with potential edge cases\n",
        "    A = expZ / (np.sum(expZ, axis=0, keepdims=True) + 1e-5)\n",
        "\n",
        "    return A\n",
        "\n",
        "def forward_prop(W1, b1, W2, b2, X):\n",
        "    '''\n",
        "    Performs forward propagation, which is the process of feeding data\n",
        "    through the network and getting a prediction, which we can then measure the\n",
        "    error on via backprop.\n",
        "\n",
        "    Returns intermediate and final value from forward prop operation.\n",
        "    '''\n",
        "\n",
        "    Z1 = W1.dot(X) + b1 # Output of layer 1 (pre-activation)\n",
        "    A1 = ReLU(Z1) # Activation applied to layer 1\n",
        "    Z2 = W2.dot(A1) + b2 # Output of layer 2\n",
        "    A2 = softmax(Z2) # Classification output\n",
        "\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "def ReLU_deriv(Z):\n",
        "    '''\n",
        "    Calculates the derivative of the ReLU function.\n",
        "\n",
        "    ReLU is defined as:\n",
        "      x if x > 0\n",
        "      0 if x <= 0\n",
        "\n",
        "    Therefore, we can return a Boolean of True (1) if the value is positive, 0\n",
        "    otherwise, which equates to the slope (gradient) of the function at that\n",
        "    point.\n",
        "    '''\n",
        "\n",
        "    return Z > 0\n",
        "\n",
        "def one_hot(Y):\n",
        "    '''\n",
        "    One-hot encodes the outcome label. This ensures that the outcome label\n",
        "    is of the same shape as the number of labels, but we add a column component\n",
        "    that has a 1 in the slot of the correct label (here, 0 - 9) and zeroes\n",
        "    everywhere else.\n",
        "\n",
        "    Returns one-hot encoded array.\n",
        "    '''\n",
        "    one_hot_Y = np.zeros((Y.size, Y.max() + 1)) # (num_examples, 9 + 1)\n",
        "    one_hot_Y[np.arange(Y.size), Y] = 1 # Indexes array of zeroes and places 1\n",
        "                                        # at label position\n",
        "\n",
        "    one_hot_Y = one_hot_Y.T # Transposes matrix for mat mul\n",
        "\n",
        "    return one_hot_Y\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
        "  '''\n",
        "  Perform backpropagation using chain rule.\n",
        "  Takes weights, biases, output for layer 1 and two,\n",
        "  as well as the activation functions performed on said layers.\n",
        "\n",
        "  Uses the number of training examples, m, for derivative calcs.\n",
        "\n",
        "  For derivative of biases for layers 1 and two, I use keepdims\n",
        "  to ensure that the resulting gradient has the same shape as b2 and b1.\n",
        "\n",
        "  Returns gradients for weights and biases for both layers in network.\n",
        "  '''\n",
        "\n",
        "  m = X.shape[1]  # Number of training examples\n",
        "  one_hot_Y = one_hot(Y) # One-hot encode label vector\n",
        "\n",
        "  dZ2 = A2 - one_hot_Y # Gradient of the output w/ respect to the true label\n",
        "  dW2 = 1 / m * dZ2.dot(A1.T) # Gradient of the weight matrix wrt the layer 2\n",
        "  db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True) # Gradient of the bias wrt layer 2\n",
        "  dZ1 = W2.T.dot(dZ2) * (Z1 > 0) # gradient of layer one output wrt activation\n",
        "                                 # of layer one and the impact of the layer 2 weight matrix\n",
        "                                 # on the layer 2 output.\n",
        "\n",
        "  dW1 = 1 / m * dZ1.dot(X.T)  # gradient of the layer 1 weight matrix wrt the layer 1\n",
        "                              # output and input\n",
        "\n",
        "  db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True) # gradient of layer 1 bias\n",
        "                                                   # wrt layer 1 output\n",
        "\n",
        "  return dW1, db1, dW2, db2\n",
        "\n",
        "\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    '''\n",
        "    Performs gradient descent step. Here, we modify the weights and biases\n",
        "    based upon the errors calculated via backprop.\n",
        "\n",
        "    The weights are adjusted based upon the learning rate to help ensure\n",
        "    smooth updates across training.\n",
        "    '''\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1\n",
        "    W2 = W2 - alpha * dW2\n",
        "    b2 = b2 - alpha * db2\n",
        "\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "rOAdbYKJljh9"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(A2):\n",
        "    '''\n",
        "    Returns the highest output from the softmax function.\n",
        "    '''\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    '''\n",
        "    Calculates accuracy of the model predictions.\n",
        "\n",
        "    Also showcases model preds alongside true labels.\n",
        "    '''\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "def gradient_descent(X, Y, alpha, iterations):\n",
        "    '''\n",
        "    Performs gradient descent.\n",
        "\n",
        "    First, it grabs initial values for weight & bias matrices.\n",
        "    Then, for how many specified iterations, it performs forward prop, then back\n",
        "    prop, then a weight and bias update.\n",
        "\n",
        "    Every ten iterations, it will show current model accuracy on its forward\n",
        "    prop run.\n",
        "\n",
        "    Returns updated params for weight & bias matrices.\n",
        "    '''\n",
        "    W1, b1, W2, b2 = init_params()\n",
        "    for i in range(iterations):\n",
        "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "        if i % 10 == 0:\n",
        "            print(\"Iteration: \", i)\n",
        "            predictions = get_predictions(A2)\n",
        "            print(get_accuracy(predictions, Y))\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "f5iv3bMDlmuz"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Gradient Descent"
      ],
      "metadata": {
        "id": "a3EoPY9floQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform gradient descent / train model\n",
        "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmx4Q-HNlrEL",
        "outputId": "cf41d005-380b-44ff-dc2c-c07e807d9ee8"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0\n",
            "[9 9 9 ... 9 9 9] [1 5 7 ... 9 1 7]\n",
            "0.10008474576271187\n",
            "Iteration:  10\n",
            "[6 7 0 ... 8 2 7] [1 5 7 ... 9 1 7]\n",
            "0.1450677966101695\n",
            "Iteration:  20\n",
            "[8 7 0 ... 8 8 7] [1 5 7 ... 9 1 7]\n",
            "0.2086271186440678\n",
            "Iteration:  30\n",
            "[1 7 0 ... 8 8 7] [1 5 7 ... 9 1 7]\n",
            "0.27177966101694917\n",
            "Iteration:  40\n",
            "[1 0 0 ... 8 1 7] [1 5 7 ... 9 1 7]\n",
            "0.33559322033898303\n",
            "Iteration:  50\n",
            "[1 0 0 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.39961016949152545\n",
            "Iteration:  60\n",
            "[1 0 0 ... 9 1 5] [1 5 7 ... 9 1 7]\n",
            "0.45796610169491525\n",
            "Iteration:  70\n",
            "[1 0 0 ... 9 1 5] [1 5 7 ... 9 1 7]\n",
            "0.513542372881356\n",
            "Iteration:  80\n",
            "[1 5 0 ... 9 1 8] [1 5 7 ... 9 1 7]\n",
            "0.5601525423728814\n",
            "Iteration:  90\n",
            "[1 5 7 ... 9 1 8] [1 5 7 ... 9 1 7]\n",
            "0.6003728813559323\n",
            "Iteration:  100\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.6331525423728813\n",
            "Iteration:  110\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.6596440677966102\n",
            "Iteration:  120\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.6835593220338984\n",
            "Iteration:  130\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7044915254237288\n",
            "Iteration:  140\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7206949152542372\n",
            "Iteration:  150\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7336610169491525\n",
            "Iteration:  160\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7448474576271187\n",
            "Iteration:  170\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7542881355932204\n",
            "Iteration:  180\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7628474576271187\n",
            "Iteration:  190\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7698305084745762\n",
            "Iteration:  200\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7764237288135594\n",
            "Iteration:  210\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7829152542372881\n",
            "Iteration:  220\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7884237288135594\n",
            "Iteration:  230\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7936271186440678\n",
            "Iteration:  240\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.7976610169491526\n",
            "Iteration:  250\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8018135593220339\n",
            "Iteration:  260\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8059152542372882\n",
            "Iteration:  270\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8098813559322033\n",
            "Iteration:  280\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8136440677966101\n",
            "Iteration:  290\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8168305084745763\n",
            "Iteration:  300\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8197118644067797\n",
            "Iteration:  310\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8229322033898305\n",
            "Iteration:  320\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8255593220338983\n",
            "Iteration:  330\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8275593220338983\n",
            "Iteration:  340\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8303559322033899\n",
            "Iteration:  350\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8326440677966102\n",
            "Iteration:  360\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.834728813559322\n",
            "Iteration:  370\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8363728813559322\n",
            "Iteration:  380\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8384915254237288\n",
            "Iteration:  390\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8403220338983051\n",
            "Iteration:  400\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8423559322033898\n",
            "Iteration:  410\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8441355932203389\n",
            "Iteration:  420\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8455423728813559\n",
            "Iteration:  430\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8473559322033898\n",
            "Iteration:  440\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.848813559322034\n",
            "Iteration:  450\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8500847457627119\n",
            "Iteration:  460\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8513559322033898\n",
            "Iteration:  470\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8526610169491525\n",
            "Iteration:  480\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8539661016949153\n",
            "Iteration:  490\n",
            "[1 5 7 ... 9 1 7] [1 5 7 ... 9 1 7]\n",
            "0.8551186440677966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Dataset"
      ],
      "metadata": {
        "id": "s19_hHpbty5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(X, W1, b1, W2, b2):\n",
        "    '''\n",
        "    Executes forward propagation and gets predictions from the model.\n",
        "    '''\n",
        "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "    predictions = get_predictions(A2)\n",
        "    return predictions\n",
        "\n",
        "def get_accuracy_test(predictions, Y):\n",
        "    '''\n",
        "    A variant of the previous accuracy test. However, here we do not print out\n",
        "    any of the predictions. We just store the accuracy value.\n",
        "    '''\n",
        "    return np.sum(predictions == Y) / Y.size"
      ],
      "metadata": {
        "id": "19K_PamFt0Ry"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run predictions on test set\n",
        "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n",
        "\n",
        "# Output accuracy on test set\n",
        "print(\n",
        "    f'Test set accuracy: {float(get_accuracy_test(dev_predictions, Y_dev)*100)}%'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy1xbhcjt4vz",
        "outputId": "dc785534-0b29-4b23-e7f1-6ec88e9d1512"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 86.4%\n"
          ]
        }
      ]
    }
  ]
}