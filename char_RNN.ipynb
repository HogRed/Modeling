{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a char-RNN\n",
        "\n",
        "Inspired by [HOML](https://www.oreilly.com/library/view/hands-on-machine-learning/9798341607972/)"
      ],
      "metadata": {
        "id": "SVI-A-t1Npvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create training dataset"
      ],
      "metadata": {
        "id": "DQpz0umvNxZ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y-awGmEiNnvm"
      },
      "outputs": [],
      "source": [
        "# download subset of Shakespeare's works (~1/4)\n",
        "# gotten from Andrej Karpathy's char-rnn project\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "def download_shakespeare_text():\n",
        "  path = Path('datasets/shakespeare/shakespeare.txt')\n",
        "  # if not file, make dir\n",
        "  if not path.is_file():\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    # get data from url\n",
        "    url = 'https://homl.info/shakespeare'\n",
        "    urllib.request.urlretrieve(url, path)\n",
        "  return path.read_text() # grab text data\n",
        "\n",
        "# invoke func\n",
        "shakespeare_text = download_shakespeare_text()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print first couple lines\n",
        "print(shakespeare_text[:80])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "420JfJ-pOpxF",
        "outputId": "b0d56dbb-0548-47bf-8ae3-1e4a6da0ff23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "z8cgVJQRO2Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get list of chars used in text\n",
        "# turn text lowercase, make a set (unique chars),\n",
        "# then sort based upon encoded value\n",
        "vocab = sorted(set(shakespeare_text.lower()))\n",
        "\n",
        "# mash vocab together into one string\n",
        "\"\".join(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F0cAp9QeO25g",
        "outputId": "271e9702-af50-47bf-86fd-5bd01d69532b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assign token id to each character; also, create way to go between word & id\n",
        "char_to_id = {char: index for index, char in enumerate(vocab)} # dict comp\n",
        "id_to_char = {index: char for index, char in enumerate(vocab)}\n",
        "\n",
        "print(char_to_id['a'])\n",
        "print(id_to_char[13])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIybjeHrPWLp",
        "outputId": "435ffd8a-4daa-440d-e3e5-1e1989176793"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## helper funcs to encode text to tensors of token ids and to decode them\n",
        "## back to text\n",
        "import torch\n",
        "\n",
        "def encode_text(text):\n",
        "  # pass list comp to tensor\n",
        "  return torch.tensor([char_to_id[char] for char in text.lower()])\n",
        "\n",
        "def decode_text(char_ids):\n",
        "  return \"\".join([id_to_char[char_id.item()] for char_id in char_ids])"
      ],
      "metadata": {
        "id": "TN4jzaDWPn9k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## test it out\n",
        "encoded = encode_text('Hello, world!')\n",
        "\n",
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDgPLrqTQRkn",
        "outputId": "745a0f82-e0a9-4490-ee35-91d87c3a7e25"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([20, 17, 24, 24, 27,  6,  1, 35, 27, 30, 24, 16,  2])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_text(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FAmP-WLIQVs5",
        "outputId": "3c40dcad-a058-4996-9204-a7acc8261b02"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello, world!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dataset"
      ],
      "metadata": {
        "id": "-GzgfZsAQeuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import (Dataset,\n",
        "                              DataLoader)\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "  # dataset initialization\n",
        "  def __init__(self, text, window_length):\n",
        "    self.encoded_text = encode_text(text)\n",
        "    self.window_length = window_length\n",
        "\n",
        "  def __len__(self):\n",
        "    # get length of dataset (have to remove window length)\n",
        "    # remember, this is a sliding window\n",
        "    return len(self.encoded_text) - self.window_length\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if idx >= len(self):\n",
        "      raise IndexError('dataset index out of range') # id beyond range of data\n",
        "    # ending points = to current idx + window length of data\n",
        "    end = idx + self.window_length\n",
        "    window = self.encoded_text[idx : end]\n",
        "    # shift by 1\n",
        "    target = self.encoded_text[idx + 1: end + 1]\n",
        "    return window, target"
      ],
      "metadata": {
        "id": "Fbge4wx6Qf-t"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create data loaders\n",
        "window_length = 50\n",
        "batch_size = 256\n",
        "\n",
        "train_set = CharDataset(shakespeare_text[:1_000_000], window_length)\n",
        "valid_set = CharDataset(shakespeare_text[1_000_000:1_060_000], window_length)\n",
        "test_set = CharDataset(shakespeare_text[1_060_000:], window_length)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "s2YbIRxXRa8b"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and Training char-RNN Model"
      ],
      "metadata": {
        "id": "w1afMySyTp5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class ShakespeareModel(nn.Module):\n",
        "  def __init__(self, vocab_size, n_layers = 2, embed_dim = 10, hidden_dim = 128,\n",
        "               dropout = 0.1):\n",
        "    super().__init__()\n",
        "    # embedding matrix\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "    # gru (rnn) layers\n",
        "    self.gru = nn.GRU(embed_dim, hidden_dim, n_layers, dropout = dropout,\n",
        "                      batch_first = True)\n",
        "    # output layer\n",
        "    self.output = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  # create forward pass functionality\n",
        "  def forward(self,X):\n",
        "    embeddings = self.embed(X) # get embeddings\n",
        "\n",
        "    outputs, _states = self.gru(embeddings) # get outputs\n",
        "\n",
        "    logits = self.output(outputs).permute(0,2,1) # get logits; swap 2 & 1 dims\n",
        "                                                 # need class dim to be 2 dim\n",
        "    return logits\n",
        "\n",
        "torch.manual_seed(42) # set seed\n",
        "\n",
        "# detect device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# create instance of model\n",
        "model = ShakespeareModel(len(vocab)).to(device)"
      ],
      "metadata": {
        "id": "yhAHIY3qTsvH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7mIoxi3Vumy",
        "outputId": "3f5640c1-d253-4233-cb04-9f70d87323ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "\n",
        "def evaluate_tm(model, data_loader, metric):\n",
        "    '''evaluates model on data loader.'''\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            metric.update(y_pred, y_batch)\n",
        "    return metric.compute()\n",
        "\n",
        "def train(model, optimizer, loss_fn, metric, train_loader, valid_loader,\n",
        "          n_epochs, patience=2, factor=0.5, epoch_callback=None):\n",
        "    ''' trains model and keeps train and validation.'''\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"max\", patience=patience, factor=factor)\n",
        "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0.0\n",
        "        metric.reset()\n",
        "        model.train()\n",
        "        if epoch_callback is not None:\n",
        "            epoch_callback(model, epoch)\n",
        "        for index, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            metric.update(y_pred, y_batch)\n",
        "            train_metric = metric.compute().item()\n",
        "            print(f\"\\rBatch {index + 1}/{len(train_loader)}\", end=\"\")\n",
        "            print(f\", loss={total_loss/(index+1):.4f}\", end=\"\")\n",
        "            print(f\", {train_metric=:.2%}\", end=\"\")\n",
        "        history[\"train_losses\"].append(total_loss / len(train_loader))\n",
        "        history[\"train_metrics\"].append(train_metric)\n",
        "        val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
        "        history[\"valid_metrics\"].append(val_metric)\n",
        "        scheduler.step(val_metric)\n",
        "        print(f\"\\rEpoch {epoch + 1}/{n_epochs},                      \"\n",
        "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
        "              f\"train metric: {history['train_metrics'][-1]:.2%}, \"\n",
        "              f\"valid metric: {history['valid_metrics'][-1]:.2%}\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "q6WAi6K8VXBa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "xentropy = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.NAdam(model.parameters())\n",
        "accuracy = torchmetrics.Accuracy(task=\"multiclass\",\n",
        "                                 num_classes=len(vocab)).to(device)\n",
        "\n",
        "history = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader,\n",
        "                n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op-z4ZKoWCbg",
        "outputId": "e77e4cce-7a38-4555-d9c9-8d4036e9b07c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20,                      train loss: 1.5213, train metric: 53.30%, valid metric: 53.37%\n",
            "Epoch 2/20,                      train loss: 1.3738, train metric: 56.93%, valid metric: 53.93%\n",
            "Epoch 3/20,                      train loss: 1.3535, train metric: 57.43%, valid metric: 53.97%\n",
            "Epoch 4/20,                      train loss: 1.3408, train metric: 57.77%, valid metric: 54.33%\n",
            "Epoch 5/20,                      train loss: 1.3304, train metric: 58.05%, valid metric: 54.54%\n",
            "Epoch 6/20,                      train loss: 1.3257, train metric: 58.16%, valid metric: 54.51%\n",
            "Epoch 7/20,                      train loss: 1.3225, train metric: 58.24%, valid metric: 54.62%\n",
            "Epoch 8/20,                      train loss: 1.3200, train metric: 58.30%, valid metric: 54.66%\n",
            "Epoch 9/20,                      train loss: 1.3182, train metric: 58.34%, valid metric: 54.43%\n",
            "Epoch 10/20,                      train loss: 1.3165, train metric: 58.39%, valid metric: 54.59%\n",
            "Epoch 11/20,                      train loss: 1.3152, train metric: 58.41%, valid metric: 54.64%\n",
            "Epoch 12/20,                      train loss: 1.3027, train metric: 58.78%, valid metric: 55.07%\n",
            "Epoch 13/20,                      train loss: 1.3011, train metric: 58.82%, valid metric: 55.12%\n",
            "Epoch 14/20,                      train loss: 1.3001, train metric: 58.85%, valid metric: 54.84%\n",
            "Epoch 15/20,                      train loss: 1.2993, train metric: 58.87%, valid metric: 54.98%\n",
            "Epoch 16/20,                      train loss: 1.2987, train metric: 58.88%, valid metric: 54.88%\n",
            "Epoch 17/20,                      train loss: 1.2921, train metric: 59.08%, valid metric: 55.04%\n",
            "Epoch 18/20,                      train loss: 1.2912, train metric: 59.10%, valid metric: 55.09%\n",
            "Epoch 19/20,                      train loss: 1.2908, train metric: 59.11%, valid metric: 55.02%\n",
            "Epoch 20/20,                      train loss: 1.2872, train metric: 59.21%, valid metric: 55.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## use trained model to predict next char in sequence\n",
        "model.eval() # no updates\n",
        "text = 'To be or not to b'\n",
        "encoded_text = encode_text(text).unsqueeze(dim=0).to(device) # add dim\n",
        "\n",
        "with torch.no_grad():\n",
        "  Y_logits = model(encoded_text)\n",
        "  predicted_char_id = Y_logits[0, :, -1].argmax().item() # get predicted next char\n",
        "  predicted_char = id_to_char[predicted_char_id]\n",
        "\n",
        "print(f'{text} -> {predicted_char}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af_8Az37Y-Mb",
        "outputId": "38c20763-9f63-440c-9e1c-00be7f65ba8f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to b -> e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Fake Shakespearean Text"
      ],
      "metadata": {
        "id": "I-1qs6x0ZdhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## use multinomial to sample indices given class probs\n",
        "torch.manual_seed(42)\n",
        "probs = torch.tensor([0.5, 0.4, 0.1])\n",
        "\n",
        "## example\n",
        "samples = torch.multinomial(probs, replacement=True, num_samples = 8)\n",
        "samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9Jtv914Zf7Y",
        "outputId": "cb452922-b229-4223-ca73-a2980a29a1e0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 1, 0, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## func to use temperature to pick next char to add to input txt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def next_char(model, text, temperature=1.0):\n",
        "  encoded_text = encode_text(text).unsqueeze(dim=0).to(device) # add dim\n",
        "\n",
        "  with torch.no_grad():\n",
        "    Y_logits = model(encoded_text)\n",
        "    # get logits for last step and div by temp\n",
        "    Y_probs = F.softmax(Y_logits[0, :, -1] / temperature, dim=-1)\n",
        "    # get predicted next char\n",
        "    predicted_char_id = torch.multinomial(Y_probs, num_samples=1).item()\n",
        "\n",
        "  return id_to_char[predicted_char_id]"
      ],
      "metadata": {
        "id": "OkJbIComZ7kw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# func to repreatedly call next_char() and append char to text\n",
        "def extend_text(model, text, n_chars=80, temperature=1.0):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(model, text, temperature)\n",
        "  return text"
      ],
      "metadata": {
        "id": "iKYKZv0naUQN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## low temp\n",
        "print(extend_text(model, 'To be or not to b', temperature=0.01))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffrehctkadN7",
        "outputId": "79d0725c-0a4f-4098-f05a-c1d5eba2feb0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be the state,\n",
            "and the state and the state and the state and the state and the sta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## medium temp\n",
        "print(extend_text(model, 'To be or not to b', temperature=0.4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_dfSRsaai6e",
        "outputId": "25e15afd-bd67-49ab-d1b7-e734371ed38d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be so far\n",
            "and be not the heavens will sit and end the first stay with my death\n",
            "of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## high temp\n",
        "print(extend_text(model, 'To be or not to b', temperature=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r445sj9PammQ",
        "outputId": "16e21224-c3fb-4f66-c058-5e125e819711"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to bmhf:my:r,k;s-h cqvvnfnfsut&-oq'ryoeen?x-hp:d,y&wv f3,dzrdzj-p$lv?xpzc,fborp;'?$u\n"
          ]
        }
      ]
    }
  ]
}