{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "UCI Data can be found [here](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset).\n",
        "\n",
        "Notebook inspired by [Hands-On Machine Learning with Scikit-Learn and PyTorch](https://www.oreilly.com/library/view/hands-on-machine-learning/9798341607972/)."
      ],
      "metadata": {
        "id": "GHHuEVeDRNxk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f3OejhXAOhTG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dat = pd.read_csv('day.csv')"
      ],
      "metadata": {
        "id": "AQ3ML4lpQCYm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = dat.drop(columns = ['cnt', 'instant','dteday'])\n",
        "y = dat['cnt']"
      ],
      "metadata": {
        "id": "SfaTdA8qQEYL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train & test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 501)\n",
        "\n",
        "# create validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 501)"
      ],
      "metadata": {
        "id": "tiMvCa27QFbh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# center and scale X data\n",
        "X_train = torch.FloatTensor(X_train.to_numpy())\n",
        "X_valid = torch.FloatTensor(X_val.to_numpy())\n",
        "X_test = torch.FloatTensor(X_test.to_numpy())\n",
        "\n",
        "means = X_train.mean(dim=0, keepdims=True)\n",
        "stds = X_train.std(dim=0, keepdims=True)\n",
        "\n",
        "X_train = (X_train - means) / stds\n",
        "X_valid = (X_valid - means) / stds\n",
        "X_test = (X_test - means) / stds"
      ],
      "metadata": {
        "id": "65rDmV47QHrq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# center and scale y data\n",
        "y_train = torch.FloatTensor(y_train.to_numpy()).reshape(-1,1)\n",
        "y_valid = torch.FloatTensor(y_val.to_numpy()).reshape(-1,1)\n",
        "y_test = torch.FloatTensor(y_test.to_numpy()).reshape(-1,1)\n",
        "\n",
        "y_mean = y_train.mean(dim=0, keepdims=True)\n",
        "y_std  = y_train.std(dim=0, keepdims=True)\n",
        "\n",
        "y_train = (y_train - y_mean) / y_std\n",
        "y_valid = (y_valid - y_mean) / y_std\n",
        "y_test  = (y_test  - y_mean) / y_std"
      ],
      "metadata": {
        "id": "iVy8R6dSQI47"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helpers"
      ],
      "metadata": {
        "id": "5MEhz16rRPiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bgd(model, optimizer, criterion, X_train, y_train, n_epochs):\n",
        "  for epoch in range(n_epochs):\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train) # get loss val\n",
        "    loss.backward() # calc grads\n",
        "    optimizer.step() # take grad desc step\n",
        "    optimizer.zero_grad() # zero out grads for next pass\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "# train function to implement mb gd\n",
        "def train_mbgd(model, optimizer, criterion, train_loader, n_epochs):\n",
        "  model.train() # set training mode\n",
        "  for epoch in range(n_epochs):\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "      # get batch\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "      # mod pred\n",
        "      y_pred = model(X_batch)\n",
        "      # calc loss and tally\n",
        "      loss = criterion(y_pred, y_batch)\n",
        "      total_loss += loss.item()\n",
        "      # calc grads and do step\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    mean_loss = total_loss / len(train_loader)\n",
        "    if epoch % 10 == 0: # every ten epochs, print out loss\n",
        "      print(f'Epoch {epoch + 1}, Loss: {mean_loss}')\n",
        "\n",
        "## create evaluation function\n",
        "def evaluate(model, data_loader, metric, aggregate = torch.mean):\n",
        "  model.eval() # change model mode to evaluation (no gradient work)\n",
        "  metrics = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_batch, y_batch in data_loader:\n",
        "      # move data to GPU / cuda\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "      y_pred = model(X_batch)\n",
        "      metric_val = metric(y_pred, y_batch)\n",
        "      metrics.append(metric_val)\n",
        "\n",
        "  # retrun agg met over all batches\n",
        "  return aggregate(torch.stack(metrics))"
      ],
      "metadata": {
        "id": "-08IblLxQKaq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device depending on what's available\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = 'mps'\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "metadata": {
        "id": "4joK5q92QMjv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = X_train.shape[1] # get cols"
      ],
      "metadata": {
        "id": "HrD3ucz9QYw7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Wide and Deep Network"
      ],
      "metadata": {
        "id": "4gUmUGGHPps8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create a custom pytorch class for a wide and deep nn\n",
        "class WideAndDeep(nn.Module):\n",
        "  def __init__(self, n_features):\n",
        "    super().__init__() # initialize from nn module parent\n",
        "    # create nn sequential stack / layers\n",
        "    self.deep_stack = nn.Sequential(\n",
        "        nn.Linear(n_features, 50),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(50,40),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    # define model output\n",
        "    self.output_layer = nn.Linear(40 + n_features, 1)\n",
        "\n",
        "    # define forward pass\n",
        "  def forward(self, X):\n",
        "    deep_output = self.deep_stack(X) # pass data into stack\n",
        "    wide_and_deep = torch.concat([X, deep_output], dim = 1) # output nn output plus og input\n",
        "    return self.output_layer(wide_and_deep) # pass through output layer"
      ],
      "metadata": {
        "id": "rGzrdnquOtVa"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and Instance of Custom Module"
      ],
      "metadata": {
        "id": "mchATuINPr1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed\n",
        "torch.manual_seed(501)\n",
        "\n",
        "# create mod instance\n",
        "model = WideAndDeep(n_features).to(device)\n",
        "\n",
        "# define learning rate\n",
        "learning_rate = 0.002"
      ],
      "metadata": {
        "id": "vyAPVguZPpAN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set model training params\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "n_epochs = 100"
      ],
      "metadata": {
        "id": "NTSL8pMWQ4SD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle = True)"
      ],
      "metadata": {
        "id": "TSEzDvQ0RAfk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mbgd(model, optimizer, criterion, train_loader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF-y45ImRBGd",
        "outputId": "675eee01-6749-402e-8639-572c530a7de2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8037662327289581\n",
            "Epoch 11, Loss: 0.08511834293603897\n",
            "Epoch 21, Loss: 0.04591403106848399\n",
            "Epoch 31, Loss: 0.03313921789328257\n",
            "Epoch 41, Loss: 0.026537391170859338\n",
            "Epoch 51, Loss: 0.02094374137620131\n",
            "Epoch 61, Loss: 0.016860316569606463\n",
            "Epoch 71, Loss: 0.013902504183351993\n",
            "Epoch 81, Loss: 0.01142065618187189\n",
            "Epoch 91, Loss: 0.009896273693690697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up validation data loader\n",
        "# don't typically use shuffle in evaluation so it's deterministic\n",
        "# and the ordering is stable\n",
        "valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle = False)\n",
        "\n",
        "# RMSE on validation data\n",
        "evaluate(model, valid_loader, criterion,\n",
        "         aggregate = lambda metrics: torch.sqrt(torch.mean(metrics)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVpzoqNgRRCX",
        "outputId": "d53387af-812c-4161-c1ff-8a5f678b57c6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1137)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wide and Deep V2"
      ],
      "metadata": {
        "id": "92v-LpYFRb-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new version which sends some features through the wide path\n",
        "# and others through the deep path\n",
        "class WideAndDeepV2(nn.Module):\n",
        "  def __init__(self, n_features):\n",
        "    super().__init__() # initialize from nn module parent\n",
        "    # create nn sequential stack / layers\n",
        "    self.deep_stack = nn.Sequential(\n",
        "        nn.Linear(n_features - 2, 50), # adjust for deep change\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(50,40),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    # define model output\n",
        "    self.output_layer = nn.Linear(40 + 5, 1) # adjust for wide change\n",
        "\n",
        "    # define forward pass\n",
        "  def forward(self, X):\n",
        "    X_wide = X[:, :5]\n",
        "    X_deep = X[:, 2:]\n",
        "    deep_output = self.deep_stack(X_deep) # pass data into stack\n",
        "    wide_and_deep = torch.concat([X_wide, deep_output], dim = 1) # output nn output plus og input\n",
        "    return self.output_layer(wide_and_deep) # pass through output layer"
      ],
      "metadata": {
        "id": "GnbEGBhJRdK8"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed\n",
        "torch.manual_seed(501)\n",
        "\n",
        "# create mod instance\n",
        "model = WideAndDeepV2(n_features).to(device)\n",
        "\n",
        "# define learning rate\n",
        "learning_rate = 0.002"
      ],
      "metadata": {
        "id": "uztO7I5-SiM_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set model training params\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "n_epochs = 100\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle = True)"
      ],
      "metadata": {
        "id": "-aLgwUsDSnto"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mbgd(model, optimizer, criterion, train_loader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzNsKhViSrEH",
        "outputId": "00d6216a-d60c-4f23-8137-1fb671f1c7fe"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8754307985305786\n",
            "Epoch 11, Loss: 0.4230686555306117\n",
            "Epoch 21, Loss: 0.250144832332929\n",
            "Epoch 31, Loss: 0.14925698985656102\n",
            "Epoch 41, Loss: 0.1075581689675649\n",
            "Epoch 51, Loss: 0.08855915988485018\n",
            "Epoch 61, Loss: 0.08018558671077093\n",
            "Epoch 71, Loss: 0.07391772791743279\n",
            "Epoch 81, Loss: 0.06863964820901552\n",
            "Epoch 91, Loss: 0.06610908806324005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up validation data loader\n",
        "# don't typically use shuffle in evaluation so it's deterministic\n",
        "# and the ordering is stable\n",
        "valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle = False)\n",
        "\n",
        "# RMSE on validation data\n",
        "evaluate(model, valid_loader, criterion,\n",
        "         aggregate = lambda metrics: torch.sqrt(torch.mean(metrics)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0toYpiHTHDQ",
        "outputId": "c234e759-89b9-4d4d-9efc-0cc4d94259c0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2906)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models with Multiple Inputs"
      ],
      "metadata": {
        "id": "NFg1i1TCTIOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WideAndDeepV3(nn.Module):\n",
        "  def __init__(self, n_features):\n",
        "    super().__init__() # initialize from nn module parent\n",
        "    # create nn sequential stack / layers\n",
        "    self.deep_stack = nn.Sequential(\n",
        "        nn.Linear(n_features - 2, 50), # adjust for deep change\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(50,40),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    # define model output\n",
        "    self.output_layer = nn.Linear(40 + 5, 1) # adjust for wide change\n",
        "\n",
        "    # define forward pass\n",
        "  def forward(self, X_wide, X_deep):\n",
        "    deep_output = self.deep_stack(X_deep) # pass data into stack\n",
        "    wide_and_deep = torch.concat([X_wide, deep_output], dim = 1) # output nn output plus og input\n",
        "    return self.output_layer(wide_and_deep)"
      ],
      "metadata": {
        "id": "jjO49MjLTKtX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create custom dataset to handle returning three tensors\n",
        "class WideAndDeepDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X_wide, X_deep, y):\n",
        "    self.X_wide = X_wide\n",
        "    self.X_deep = X_deep\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    input_dict = {\"X_wide\": self.X_wide[idx], \"X_deep\": self.X_deep[idx]}\n",
        "    return input_dict, self.y[idx]"
      ],
      "metadata": {
        "id": "1_YAWJsRTX1P"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create datasets and data loaders\n",
        "train_data_named = WideAndDeepDataset(X_wide = X_train[:, :5], X_deep = X_train[:, 2:], y = y_train)\n",
        "train_loader_named = DataLoader(train_data_named, batch_size=32, shuffle = True)\n",
        "\n",
        "valid_data_named = WideAndDeepDataset(X_wide = X_valid[:, :5], X_deep = X_valid[:, 2:], y = y_valid)\n",
        "valid_loader_named = DataLoader(valid_data_named, batch_size=32, shuffle = False)\n",
        "\n",
        "test_data_named = WideAndDeepDataset(X_wide = X_test[:, :5], X_deep = X_test[:, 2:], y = y_test)\n",
        "test_loader_named = DataLoader(test_data_named, batch_size=32, shuffle = False)"
      ],
      "metadata": {
        "id": "0d39FdaYTwov"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## update main loop in eval and train functions\n",
        "# train function to implement mb gd\n",
        "def train_mbgd(model, optimizer, criterion, train_loader, n_epochs):\n",
        "  model.train() # set training mode\n",
        "  for epoch in range(n_epochs):\n",
        "    total_loss = 0\n",
        "    for inputs, y_batch in train_loader:\n",
        "      # get batch\n",
        "      inputs = {name: X.to(device) for name, X in inputs.items()}\n",
        "      y_batch = y_batch.to(device)\n",
        "      # mod pred\n",
        "      y_pred = model(X_wide = inputs['X_wide'], X_deep = inputs['X_deep'])\n",
        "      # calc loss and tally\n",
        "      loss = criterion(y_pred, y_batch)\n",
        "      total_loss += loss.item()\n",
        "      # calc grads and do step\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    mean_loss = total_loss / len(train_loader)\n",
        "    if epoch % 10 == 0: # every ten epochs, print out loss\n",
        "      print(f'Epoch {epoch + 1}, Loss: {mean_loss}')\n",
        "\n",
        "## create evaluation function\n",
        "def evaluate(model, data_loader, metric, aggregate = torch.mean):\n",
        "  model.eval() # change model mode to evaluation (no gradient work)\n",
        "  metrics = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, y_batch in data_loader:\n",
        "      # get batch\n",
        "      inputs = {name: X.to(device) for name, X in inputs.items()}\n",
        "      y_batch = y_batch.to(device)\n",
        "      # mod pred\n",
        "      y_pred = model(X_wide = inputs['X_wide'], X_deep = inputs['X_deep'])\n",
        "      metric_val = metric(y_pred, y_batch)\n",
        "      metrics.append(metric_val)\n",
        "\n",
        "  # retrun agg met over all batches\n",
        "  return aggregate(torch.stack(metrics))"
      ],
      "metadata": {
        "id": "shD0nt9kUZnI"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed\n",
        "torch.manual_seed(501)\n",
        "\n",
        "# create mod instance\n",
        "model = WideAndDeepV3(n_features).to(device)\n",
        "\n",
        "# define learning rate\n",
        "learning_rate = 0.002\n",
        "\n",
        "# set model training params\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "n_epochs = 100\n",
        "\n",
        "train_mbgd(model, optimizer, criterion, train_loader_named, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDhFaKC5U50T",
        "outputId": "ab5b7a10-60af-41c9-c3b9-d007f0dce4e6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8754307985305786\n",
            "Epoch 11, Loss: 0.4230686555306117\n",
            "Epoch 21, Loss: 0.250144832332929\n",
            "Epoch 31, Loss: 0.14925698985656102\n",
            "Epoch 41, Loss: 0.1075581689675649\n",
            "Epoch 51, Loss: 0.08855915988485018\n",
            "Epoch 61, Loss: 0.08018558671077093\n",
            "Epoch 71, Loss: 0.07391772791743279\n",
            "Epoch 81, Loss: 0.06863964820901552\n",
            "Epoch 91, Loss: 0.06610908806324005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSE on validation data\n",
        "evaluate(model, valid_loader_named, criterion,\n",
        "         aggregate = lambda metrics: torch.sqrt(torch.mean(metrics)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fvmBPwpWR6S",
        "outputId": "88b4dc81-a917-4b11-b7a4-f0bc23a27aa5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2906)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models with Multiple Outputs"
      ],
      "metadata": {
        "id": "k78SoxTrXHpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## adding auxilary output to act as a form of regularization\n",
        "class WideAndDeepV4(nn.Module):\n",
        "  def __init__(self, n_features):\n",
        "    super().__init__() # initialize from nn module parent\n",
        "    # create nn sequential stack / layers\n",
        "    self.deep_stack = nn.Sequential(\n",
        "        nn.Linear(n_features - 2, 50), # adjust for deep change\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(50, 40), # 50 inputs, 40 outputs\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    # define model output\n",
        "    self.output_layer = nn.Linear(40 + 5, 1) # adjust for wide change\n",
        "\n",
        "    self.aux_output_layer = nn.Linear(40, 1)\n",
        "\n",
        "  def forward(self, X_wide, X_deep):\n",
        "    deep_output = self.deep_stack(X_deep) # pass data into stack\n",
        "    wide_and_deep = torch.concat([X_wide, deep_output], dim = 1) # output nn output plus og input\n",
        "\n",
        "    main_output = self.output_layer(wide_and_deep)\n",
        "    aux_output = self.aux_output_layer(deep_output)\n",
        "\n",
        "    return main_output, aux_output"
      ],
      "metadata": {
        "id": "3zqhLaSVXJGP"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## update main loop in training function to accomodate aux layer\n",
        "def train_mbgd(model, optimizer, criterion, train_loader, n_epochs):\n",
        "  model.train() # set training mode\n",
        "  for epoch in range(n_epochs):\n",
        "    total_loss = 0\n",
        "    for inputs, y_batch in train_loader:\n",
        "      # get batch\n",
        "      inputs = {name: X.to(device) for name, X in inputs.items()}\n",
        "      y_batch = y_batch.to(device)\n",
        "      # mod pred\n",
        "      # unpack outputs from dictionary\n",
        "      #y_pred, y_pred_aux = model(X_wide = inputs['X_wide'], X_deep = inputs['X_deep'])\n",
        "\n",
        "      y_pred, y_pred_aux = model(**inputs) # easy way to unpack multiple inputs\n",
        "\n",
        "      # get loss vals for each output\n",
        "      main_loss = criterion(y_pred, y_batch)\n",
        "      aux_loss = criterion(y_pred_aux, y_batch)\n",
        "\n",
        "      loss = 0.8 * main_loss + 0.2 * aux_loss # 80/20 weight; essentially regularization\n",
        "      total_loss += loss.item()\n",
        "      # calc grads and do step\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    mean_loss = total_loss / len(train_loader)\n",
        "    if epoch % 10 == 0: # every ten epochs, print out loss\n",
        "      print(f'Epoch {epoch + 1}, Loss: {mean_loss}')"
      ],
      "metadata": {
        "id": "uB0yY3iSYk4k"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## update main loop in eval function\n",
        "def evaluate(model, data_loader, metric, aggregate = torch.mean):\n",
        "  model.eval() # change model mode to evaluation (no gradient work)\n",
        "  metrics = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, y_batch in data_loader:\n",
        "      # get batch\n",
        "      inputs = {name: X.to(device) for name, X in inputs.items()}\n",
        "      y_batch = y_batch.to(device)\n",
        "      # mod pred\n",
        "      #y_pred, _ = model(X_wide = inputs['X_wide'], X_deep = inputs['X_deep'])\n",
        "\n",
        "      y_pred, _ = model(**inputs) # easy way to unpack multiple inputs\n",
        "\n",
        "      metric_val = metric(y_pred, y_batch)\n",
        "      metrics.append(metric_val)\n",
        "\n",
        "  # retrun agg met over all batches\n",
        "  return aggregate(torch.stack(metrics))"
      ],
      "metadata": {
        "id": "50fBjCbxZGk_"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed\n",
        "torch.manual_seed(501)\n",
        "\n",
        "# create mod instance\n",
        "model = WideAndDeepV4(n_features).to(device)\n",
        "\n",
        "# define learning rate\n",
        "learning_rate = 0.002\n",
        "\n",
        "# set model training params\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "n_epochs = 100\n",
        "\n",
        "train_mbgd(model, optimizer, criterion, train_loader_named, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvoi70FnZWe-",
        "outputId": "97b8244c-cbf8-4c2f-e633-21f284ec9b6b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9135758399963378\n",
            "Epoch 11, Loss: 0.6055165727933248\n",
            "Epoch 21, Loss: 0.4511197785536448\n",
            "Epoch 31, Loss: 0.34409328798453015\n",
            "Epoch 41, Loss: 0.2715859274069468\n",
            "Epoch 51, Loss: 0.22237140834331512\n",
            "Epoch 61, Loss: 0.18544450104236604\n",
            "Epoch 71, Loss: 0.15996836225191752\n",
            "Epoch 81, Loss: 0.13978618284066519\n",
            "Epoch 91, Loss: 0.12438217649857203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSE on validation data\n",
        "print('RMSE of V4 on validation data:', round(\n",
        "    (evaluate(model, valid_loader_named, criterion,\n",
        "         aggregate = lambda metrics: torch.sqrt(torch.mean(metrics)))).item(),\n",
        "    5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F2Y4uHCZcIP",
        "outputId": "341dda82-3a9d-4502-eeb0-2c0d12332646"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE of V4 on validation data: 0.30504\n"
          ]
        }
      ]
    }
  ]
}