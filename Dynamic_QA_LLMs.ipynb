{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "pSoMk3gOZdNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "xMcPNf7zZmpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "17HiMe0sZ3aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Package imports\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import WikipediaLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "from transformers import logging as transformers_logging\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Suppress unnecessary warning messages from Hugging Face\n",
        "transformers_logging.set_verbosity_error()\n",
        "\n",
        "# Dynamic Keyword Extraction\n",
        "def extract_keywords(query):\n",
        "    \"\"\"\n",
        "    Extracts named entities (keywords) from a query using a Named Entity\n",
        "    Recognition (NER) model.\n",
        "    Filters entities of type ORGANIZATION (ORG), LOCATION (LOC), PERSON (PER),\n",
        "    or DATE (DATE).\n",
        "    \"\"\"\n",
        "    # Use a pre-trained BERT-based NER model for named entity recognition\n",
        "    keyword_extractor = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
        "\n",
        "    # Extract named entities from the query\n",
        "    entities = keyword_extractor(query)\n",
        "\n",
        "    # Filter and return entities matching the specified types\n",
        "    return [entity['word'] for entity in entities if entity['entity'] in {\"ORG\", \"LOC\", \"PER\", \"DATE\"}]\n",
        "\n",
        "# Refined Retrieval Process\n",
        "def retrieve_relevant_context(vector_store, query, num_docs=5):\n",
        "    \"\"\"\n",
        "    Retrieves relevant documents from the FAISS vector store based on the query\n",
        "    and extracted keywords.\n",
        "    Combines the query with extracted keywords to improve relevance during retrieval.\n",
        "    \"\"\"\n",
        "    # Extract keywords from the query\n",
        "    keywords = extract_keywords(query)\n",
        "\n",
        "    # Retrieve documents from the vector store\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": num_docs})\n",
        "\n",
        "    # Combine query and keywords to improve retrieval\n",
        "    docs = retriever.get_relevant_documents(query + \" \" + \" \".join(keywords))\n",
        "\n",
        "    # Return the concatenated page content of retrieved documents\n",
        "    return \" \".join([doc.page_content for doc in docs])\n",
        "\n",
        "# Chunking with Relevance Filtering\n",
        "def chunk_and_score_context(context, query, max_length=512):\n",
        "    \"\"\"\n",
        "    Splits the retrieved context into smaller chunks and scores each chunk's\n",
        "    relevance to the query.\n",
        "    Uses SentenceTransformer embeddings and cosine similarity for scoring.\n",
        "    \"\"\"\n",
        "    # Load a pre-trained model for computing embeddings and similarity\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Split the context into chunks of max_length\n",
        "    chunks = [context[i:i + max_length] for i in range(0, len(context), max_length)]\n",
        "\n",
        "    # Compute the query embedding\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarity scores for each chunk\n",
        "    scored_chunks = [\n",
        "        (chunk,\n",
        "         util.cos_sim(query_embedding,\n",
        "                      model.encode(chunk, convert_to_tensor=True))[0].item())\n",
        "        for chunk in chunks\n",
        "    ]\n",
        "\n",
        "    # Return the top 3 chunks sorted by relevance in descending order\n",
        "    return sorted(scored_chunks, key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "# Handle Ambiguity with Confidence and Evidence\n",
        "def answer_query_with_sources(query, model, retriever):\n",
        "    \"\"\"\n",
        "    Answers a query using the retrieved context, providing the answer,\n",
        "    confidence score, and relevant sources.\n",
        "    Combines top-scoring context chunks to generate the final answer.\n",
        "    \"\"\"\n",
        "    # Retrieve relevant context for the query\n",
        "    context = retrieve_relevant_context(retriever, query)\n",
        "\n",
        "    # Select the most relevant context chunks\n",
        "    top_chunks = chunk_and_score_context(context, query)\n",
        "\n",
        "    # Combine the top chunks into a single context\n",
        "    combined_context = \" \".join([chunk[0] for chunk in top_chunks])\n",
        "\n",
        "    # Use the question-answering model to generate the answer\n",
        "    result = model(question=query, context=combined_context)\n",
        "\n",
        "    # Return the answer, confidence score, and top sources\n",
        "    return {\n",
        "        \"answer\": result['answer'],\n",
        "        \"confidence\": result['score'],\n",
        "        \"sources\": top_chunks\n",
        "    }\n",
        "\n",
        "# Initialize Components\n",
        "def initialize_model_and_retriever(query):\n",
        "    \"\"\"\n",
        "    Initializes the question-answering model, retrieves Wikipedia documents,\n",
        "    and creates a FAISS vector store for retrieval.\n",
        "    \"\"\"\n",
        "    # Load a pre-trained question-answering model and tokenizer\n",
        "    model_id = \"deepset/bert-large-uncased-whole-word-masking-squad2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
        "\n",
        "    # Set up a question-answering pipeline\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=model,\n",
        "                           tokenizer=tokenizer, device=0)\n",
        "\n",
        "    # Load Wikipedia documents relevant to the query\n",
        "    loader = WikipediaLoader(query=query, lang=\"en\")\n",
        "    try:\n",
        "        documents = loader.load()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] WikipediaLoader failed with error: {e}\")\n",
        "        documents = []\n",
        "\n",
        "    if not documents:\n",
        "        print(\"[WARNING] No documents retrieved from Wikipedia.\")\n",
        "        return qa_pipeline, None\n",
        "\n",
        "    # Create a FAISS vector store using document embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vector_store = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "    return qa_pipeline, vector_store\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the question-answering pipeline.\n",
        "    Prompts for a query, retrieves relevant context, and provides an\n",
        "    answer with confidence and sources.\n",
        "    \"\"\"\n",
        "    # MODEL QUERY ####################################\n",
        "    query = \"When did the Battle of Waterloo occur?\"\n",
        "    print(f\"[INFO] Searching Wikipedia for: {query}\")\n",
        "    ##################################################\n",
        "\n",
        "    # Initialize the model and retriever with the query\n",
        "    qa_pipeline, vector_store = initialize_model_and_retriever(query)\n",
        "\n",
        "    if vector_store is None:\n",
        "        print(\"[ERROR] Retrieval failed. Unable to proceed with the question-answering task.\")\n",
        "        return\n",
        "\n",
        "    # Get results\n",
        "    print(\"[INFO] Running question-answering...\")\n",
        "    result = answer_query_with_sources(query, qa_pipeline, vector_store)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"[INFO] Answer: {result['answer']}\")\n",
        "    print(f\"[INFO] Confidence: {result['confidence']}\")\n",
        "    print(f\"[INFO] Sources:\")\n",
        "    for source in result['sources']:\n",
        "        print(source)\n",
        "\n",
        "# Run if script is main\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kKmI4Gbt8C0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd547fc-cc8d-4075-bb60-fa8e253f068b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Searching Wikipedia for: When did the Battle of Waterloo occur?\n",
            "[INFO] Running question-answering...\n",
            "[INFO] Answer: Sunday 18 June 1815\n",
            "[INFO] Confidence: 0.6014962196350098\n",
            "[INFO] Sources:\n",
            "('piness, when she once more embraced the married state it was to marry the new landlord tavern; from which time it obtained the title it now bears.\\n\\n\\n== See also ==\\nList of Waterloo Battlefield locations\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\nGillespie-Payne, Jonathan (2003), Waterloo: In the Footsteps of the Commanders, Pen and Sword, p. 166, ISBN 978-1-4738-2060-9\\nRomberg, J. B. (1820), New picture of Brussels, p. 185 The Battle of Waterloo was fought on Sunday 18 June 1815, near Waterloo (at that time in the Un', 0.6367213726043701)\n",
            "(\"metres (9.3 mi) south of Brussels, and about 2 kilometres (1.2 mi) from the town of Waterloo. The site of the battlefield today is dominated by the monument of the Lion's Mound, a large artificial hill constructed from earth taken from the battlefield itself, but the topography of the battlefield near the mound has not  The Battle of Quatre Bras was fought on 16 June 1815, as a preliminary engagement to the decisive Battle of Waterloo that occurred two days later. The battle took place near the strategic cr\", 0.6041792631149292)\n",
            "('ng through on the French right flank, the Anglo-allied army repulsed the Imperial Guard, and the French army was routed.\\nWaterloo was the decisive engagement of the Waterloo campaign and Napoleon\\'s last. It was also the second bloodiest single day battle of the Napoleonic Wars, after Borodino. According to Wellington, the battle was \"the nearest-run thing you ever saw in your life\". Napoleon abdicated four days later, and coalition forces entered Paris on 7 July. The defeat at Waterloo marked the end of Nap', 0.5987491607666016)\n"
          ]
        }
      ]
    }
  ]
}