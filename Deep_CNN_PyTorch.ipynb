{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN's with PyTorch\n",
        "\n",
        "Notebook inspired by [Hands-On Machine Learning with Scikit-Learn and PyTorch](https://www.oreilly.com/library/view/hands-on-machine-learning/9798341607972/)."
      ],
      "metadata": {
        "id": "PS_LaJOv6X5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Load in Images"
      ],
      "metadata": {
        "id": "BX77Mg2F6a5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eiSD8I6q6UOu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.datasets import load_sample_images\n",
        "\n",
        "# load and stack images into array\n",
        "sample_images = np.stack(load_sample_images()['images'])\n",
        "\n",
        "# turn to tensor and normalize pixel vals\n",
        "sample_images = torch.tensor(sample_images, dtype=torch.float32) / 255"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 images, 427 x 640, 3 color channels\n",
        "sample_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI05lkcb67bA",
        "outputId": "9b073921-4571-4ed2-824c-32ab4a712310"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 427, 640, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move channel to be just b4 height and width\n",
        "sample_images_permuted = sample_images.permute(0, 3, 1, 2)\n",
        "\n",
        "sample_images_permuted.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXeQNY7l7B6W",
        "outputId": "495adb75-b275-46a0-e035-97340376b19d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 427, 640])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# center-crop images\n",
        "import torchvision\n",
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "cropped_images = T.CenterCrop((70,120))(sample_images_permuted)\n",
        "\n",
        "cropped_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc0dgdZe7IWT",
        "outputId": "8a609357-50b1-4fdf-e8db-01d225d166ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 70, 120])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create 2D Convolutional Layer and Feed it Cropped Images"
      ],
      "metadata": {
        "id": "mlPS8CDw7WSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conv layer w/ 32 filters, each 7x7\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(7,7)) # or could do just 7 for kernel size\n",
        "\n",
        "# feature maps\n",
        "fmaps = conv_layer(cropped_images)\n",
        "\n",
        "# output shape\n",
        "fmaps.shape\n",
        "\n",
        "# notice:\n",
        "# 32 feature maps\n",
        "# H & W shrunk by 6 pixels (no zero padding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXTxRgCC7ZjX",
        "outputId": "50589efc-56e7-4a5e-863b-077d3eb7b5fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 32, 64, 114])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want it to stay the same, add padding\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(7,7), padding='same')\n",
        "\n",
        "# feature maps\n",
        "fmaps = conv_layer(cropped_images)\n",
        "\n",
        "# output shape\n",
        "fmaps.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhwNnQub8DzP",
        "outputId": "59738e3d-d779-4bc5-aeaa-bbd6b625230c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 32, 70, 120])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# layer attributes\n",
        "print(conv_layer.weight.shape) # out chan, in chan, kernel height, kernel width\n",
        "print(conv_layer.bias.shape) # output channels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VroCS7N-8day",
        "outputId": "68dc51b4-6a5d-485c-9036-d3e9cbbd36f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 7, 7])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling Layers"
      ],
      "metadata": {
        "id": "Kgiay4MR9rON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2x2 pooling layer; default stride of 2, padding = 0 / 'valid'\n",
        "max_pool = nn.MaxPool2d(kernel_size = (2,2))"
      ],
      "metadata": {
        "id": "8Bn5Kp_DTJcx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom depth-wise pooling layer; can allow CNN to learn translation\n",
        "# invariance\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DepthPool(torch.nn.Module):\n",
        "  def __init__(self, kernel_size, stride=None, padding=0):\n",
        "    super().__init__()\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = stride if stride is not None else kernel_size\n",
        "    self.padding = padding\n",
        "\n",
        "  # forward pass\n",
        "  def forward(self, inputs):\n",
        "    batch, channels, height, width = inputs.shape\n",
        "    # merges spatial dimensions\n",
        "    z = inputs.view(batch, channels, height * width)\n",
        "\n",
        "    # swap spatial and channel dimensions\n",
        "    z = z.permute(0,2,1)\n",
        "\n",
        "    # max pool along last dimension which is the channels\n",
        "    z = F.max_pool1d(z, kernel_size=self.kernel_size,\n",
        "                     stride = self.stride, padding = self.padding)\n",
        "\n",
        "    # move back spatial & channel dimensions\n",
        "    z = z.permute(0,2,1)\n",
        "\n",
        "    # unmerge spatial dimensions\n",
        "    z = z.view(batch, -1, height, width)\n",
        "\n",
        "    return z"
      ],
      "metadata": {
        "id": "Av8m6ZZ4Tm1i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a global average pooling layer;\n",
        "# computes mean of each entire feature map... very destructive\n",
        "global_avg_pool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "\n",
        "output = global_avg_pool(cropped_images)\n",
        "\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akZKXuP6VEav",
        "outputId": "ecf85f03-c3d1-4f90-dae9-16446ef5542f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or you could do the mean to get the same output for this\n",
        "output = cropped_images.mean(dim = (2,3), keepdim=True)\n",
        "\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uLuB6VqVU4j",
        "outputId": "ef3ba380-221b-4129-a61b-33d3bb08d231"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Architectures"
      ],
      "metadata": {
        "id": "19QbL04gVdLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## basic CNN to tackle Fashion MNIST\n",
        "from functools import partial\n",
        "\n",
        "# little wrapper to allow us to reuse default arguments w/o repeating ourselves\n",
        "DefaultConv2d = partial(nn.Conv2d, kernel_size=3, padding='same')\n",
        "\n",
        "# basically stacking convolutional layers, ReLU, pooling over and over again,\n",
        "# until it's time to flatten and proceed through FFN; sprinkle in dropout\n",
        "# towards the end for regularization\n",
        "model = nn.Sequential(\n",
        "    DefaultConv2d(in_channels=1, out_channels=64, kernel_size=7),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    DefaultConv2d(in_channels=64, out_channels=128),\n",
        "    nn.ReLU(),\n",
        "    DefaultConv2d(in_channels=128, out_channels=128),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    DefaultConv2d(in_channels=128, out_channels=256),\n",
        "    nn.ReLU(),\n",
        "    DefaultConv2d(in_channels=256, out_channels=256),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Flatten(),\n",
        "    # images start out as 28x28... after pooling operations, they are down\n",
        "    # to 3x3. Multiply that by the number of feature maps (256) at this point,\n",
        "    # and you end up with 2304 input features\n",
        "    nn.Linear(in_features=2304, out_features=128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(in_features=128, out_features=64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(in_features=64, out_features=10)\n",
        ").to(device='cuda')"
      ],
      "metadata": {
        "id": "Cxlt-_9DVeZO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device depending on what's available\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = 'mps'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "# create tensor object we'll transform FashionMNIST data to\n",
        "toTensor = T.Compose([T.ToImage(), T.ToDtype(torch.float32, scale = True)])\n",
        "\n",
        "# bring in train, test, valid data\n",
        "train_and_valid_data = torchvision.datasets.FashionMNIST(\n",
        "    root = 'datasets',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = toTensor\n",
        ")\n",
        "\n",
        "test_data = torchvision.datasets.FashionMNIST(\n",
        "    root = 'datasets',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = toTensor\n",
        ")\n",
        "\n",
        "# reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# save back 5_000 from train to be reserved for validation\n",
        "train_data, valid_data = torch.utils.data.random_split(\n",
        "    train_and_valid_data,\n",
        "    [55_000, 5_000]\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# create data loaders\n",
        "train_loader = DataLoader(train_data, batch_size = 32, shuffle = True)\n",
        "valid_loader = DataLoader(valid_data, batch_size = 32)\n",
        "test_loader = DataLoader(test_data, batch_size = 32)"
      ],
      "metadata": {
        "id": "CTDRRGxXXe6T"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set model training params\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "n_epochs = 100"
      ],
      "metadata": {
        "id": "rCMSEgx9X3Lp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train function to implement mb gd\n",
        "def train_mbgd(model, optimizer, criterion, train_loader, n_epochs):\n",
        "  model.train() # set training mode\n",
        "  for epoch in range(n_epochs):\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "      # get batch\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "      # mod pred\n",
        "      y_pred = model(X_batch)\n",
        "      # calc loss and tally\n",
        "      loss = criterion(y_pred, y_batch)\n",
        "      total_loss += loss.item()\n",
        "      # calc grads and do step\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    mean_loss = total_loss / len(train_loader)\n",
        "    if epoch % 10 == 0: # every ten epochs, print out loss\n",
        "      print(f'Epoch {epoch + 1}, Loss: {mean_loss}')"
      ],
      "metadata": {
        "id": "cZVUee8GYCLO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device) # Move model to the correct device\n",
        "train_mbgd(model, optimizer, criterion, train_loader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gemrAT1_YDCJ",
        "outputId": "91671883-d8b2-49cf-ad62-cf409a6f62c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.302686505431419\n",
            "Epoch 11, Loss: 0.4731756708974267\n",
            "Epoch 21, Loss: 0.3492205565824559\n",
            "Epoch 31, Loss: 0.28776134889726723\n",
            "Epoch 41, Loss: 0.2467984036987278\n",
            "Epoch 51, Loss: 0.21463477263826317\n",
            "Epoch 61, Loss: 0.18125992719248726\n",
            "Epoch 71, Loss: 0.15336713460035656\n",
            "Epoch 81, Loss: 0.13641530665820395\n",
            "Epoch 91, Loss: 0.11388902980094714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create evaluation function\n",
        "def evaluate(model, data_loader, metric, aggregate = torch.mean):\n",
        "  model.eval() # change model mode to evaluation (no gradient work)\n",
        "  metrics = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_batch, y_batch in data_loader:\n",
        "      # move data to GPU / cuda\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "      y_pred = model(X_batch)\n",
        "      metric_val = metric(y_pred, y_batch)\n",
        "      metrics.append(metric_val)\n",
        "\n",
        "  # retrun agg met over all batches\n",
        "  return aggregate(torch.stack(metrics))"
      ],
      "metadata": {
        "id": "twQPwUW-ZXDx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS-xlG9iZXeJ",
        "outputId": "dbef8f0a-080a-4264-a1cc-c0e7e69d12b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use accuracy metric to evaluate predictive ability\n",
        "import torchmetrics\n",
        "accuracy = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10).to(device)\n",
        "\n",
        "# accuracy on validation data\n",
        "# calc batch-wise accuracy w/ lambda func\n",
        "# get average of batches via aggregate\n",
        "accuracy_val = evaluate(model, valid_loader,\n",
        "                        lambda y_pred, y_batch: (y_pred.argmax(dim=1)\n",
        "                        == y_batch).float().mean(),\n",
        "                        aggregate = torch.mean)\n",
        "\n",
        "print(f'Validation Accuracy: {accuracy_val.item()*100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-DsDNPdZY1s",
        "outputId": "cb720d67-a153-4966-f647-d0ade8b5626b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 91.1027%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separable Convolutional Layers"
      ],
      "metadata": {
        "id": "836jvdofeUWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN layer that considers spatial and cross-channel patterns separately\n",
        "# in practice, uses fewer params, less memory, fewer computations,\n",
        "# often better performance. Just don't use after layers that have\n",
        "# not a lot of channels\n",
        "class SeparableConv2d(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride = 1,\n",
        "               padding = 0):\n",
        "    super().__init__()\n",
        "    # groups = in_channels leads to depthwise convolutional layer\n",
        "    self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,\n",
        "                               stride = stride, padding = padding,\n",
        "                               groups = in_channels)\n",
        "\n",
        "    self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size = 1,\n",
        "                               stride = 1, padding = 0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      return self.pointwise(self.depthwise(inputs))"
      ],
      "metadata": {
        "id": "C_3OAWDXeWGs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet-34 CNN"
      ],
      "metadata": {
        "id": "bcMPWjO8hmh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# residual unit layer\n",
        "class ResidualUnit(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride = 1):\n",
        "    super().__init__()\n",
        "    DefaultConv2d = partial(\n",
        "        nn.Conv2d, kernel_size=3, padding=1, stride = 1, bias=False\n",
        "    )\n",
        "    self.main_layers = nn.Sequential(\n",
        "        DefaultConv2d(in_channels, out_channels, stride = stride),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(),\n",
        "        DefaultConv2d(out_channels, out_channels),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "    if stride > 1:\n",
        "      self.skip_connection = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                    stride = stride, padding = 0),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    else:\n",
        "      self.skip_connection = nn.Identity() # identity doesn't do anything; just\n",
        "                                           # return inputs\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return F.relu(self.main_layers(inputs) + self.skip_connection(inputs))"
      ],
      "metadata": {
        "id": "Bc3vN7nchoC_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building actual resnet model now; with residual unit, this makes it\n",
        "# easy to build sequential style\n",
        "class ResNet34(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    layers = [\n",
        "        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7,\n",
        "                  stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(num_features=64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    ]\n",
        "\n",
        "    # first three residual units have 64 filters\n",
        "    # next 4 have 128 and so forth...\n",
        "    prev_filters = 64\n",
        "\n",
        "    for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "      stride = 1 if filters == prev_filters else 2\n",
        "      layers.append(ResidualUnit(prev_filters, filters, stride=stride))\n",
        "      prev_filters = filters\n",
        "\n",
        "    # add RUs to the list of layers\n",
        "    # lazy linear allows us to not have to figure out shape of input\n",
        "    layers += [\n",
        "        nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
        "        nn.Flatten(),\n",
        "        nn.LazyLinear(10)\n",
        "    ]\n",
        "\n",
        "    # create overall sequential model w/ all layers\n",
        "    self.resnet = nn.Sequential(*layers)\n",
        "\n",
        "  # forward pass\n",
        "  def forward(self, x):\n",
        "    return self.resnet(x)"
      ],
      "metadata": {
        "id": "D0SoSopMiZrW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training ResNet-34"
      ],
      "metadata": {
        "id": "eJr6CV9Sl0tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bring in 10-class subset of ImageNet\n",
        "from torchvision.datasets import Imagenette\n",
        "from torchvision import transforms\n",
        "\n",
        "# perform some data augmentation\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)),\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)),\n",
        "])\n",
        "\n",
        "# load in, perform transformations\n",
        "train_ds = Imagenette(root=\"./data\", split=\"train\", size=\"160px\",\n",
        "                      download=True, transform=train_tfms)\n",
        "\n",
        "val_ds = Imagenette(root=\"./data\", split=\"val\", size=\"160px\",\n",
        "                      download=True, transform=val_tfms)\n",
        "\n",
        "model = ResNet34().to(device)"
      ],
      "metadata": {
        "id": "OqRvl4mDl2BC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data loaders\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "bjUmhaPVmZq3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up training criteria; use label smoothing to help with generalization\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# using SGD w/ Nesterov momentum, weight decay\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.05,\n",
        "    momentum=0.9,\n",
        "    weight_decay=1e-4,\n",
        "    nesterov=True\n",
        ")\n",
        "\n",
        "# cosine annealing for 40 epochs; add warm restarts\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,\n",
        "                                                       T_0=40,\n",
        "                                                       T_mult=2)"
      ],
      "metadata": {
        "id": "cRgSd5o9oPib"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "def train_mbgd(model, optimizer, criterion, train_loader, n_epochs,\n",
        "               val_loader=None):\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        mean_loss = total_loss / len(train_loader)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()  # advance schedule for next epoch\n",
        "\n",
        "        if val_loader is not None:\n",
        "            val_acc = evaluate(\n",
        "                model,\n",
        "                val_loader,\n",
        "                lambda y_pred, y: (y_pred.argmax(dim=1) == y).float().mean(),\n",
        "                aggregate=torch.mean\n",
        "            ).item()\n",
        "            print(f\"Epoch {epoch+1:03d}, train loss={mean_loss:.4f}, validation accuracy={val_acc*100:.2f}%\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1:03d}, train loss={mean_loss:.4f}\")"
      ],
      "metadata": {
        "id": "xeljsnizojIz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train! :-)\n",
        "train_mbgd(model, optimizer, criterion, train_loader, n_epochs=40,\n",
        "           val_loader=val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ThZhHgorK8",
        "outputId": "dbd51198-3508-41fe-eff7-2b04c2247152"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001, train loss=2.6230, validation accuracy=27.53%\n",
            "Epoch 002, train loss=2.0726, validation accuracy=38.02%\n",
            "Epoch 003, train loss=1.9497, validation accuracy=44.88%\n",
            "Epoch 004, train loss=1.8712, validation accuracy=49.64%\n",
            "Epoch 005, train loss=1.7805, validation accuracy=49.93%\n",
            "Epoch 006, train loss=1.7153, validation accuracy=55.65%\n",
            "Epoch 007, train loss=1.6603, validation accuracy=58.40%\n",
            "Epoch 008, train loss=1.5619, validation accuracy=56.18%\n",
            "Epoch 009, train loss=1.5095, validation accuracy=62.73%\n",
            "Epoch 010, train loss=1.4505, validation accuracy=66.11%\n",
            "Epoch 011, train loss=1.4096, validation accuracy=68.20%\n",
            "Epoch 012, train loss=1.3738, validation accuracy=61.73%\n",
            "Epoch 013, train loss=1.3508, validation accuracy=67.37%\n",
            "Epoch 014, train loss=1.3241, validation accuracy=67.22%\n",
            "Epoch 015, train loss=1.2819, validation accuracy=63.81%\n",
            "Epoch 016, train loss=1.2520, validation accuracy=74.58%\n",
            "Epoch 017, train loss=1.2422, validation accuracy=75.24%\n",
            "Epoch 018, train loss=1.2244, validation accuracy=75.12%\n",
            "Epoch 019, train loss=1.2011, validation accuracy=77.35%\n",
            "Epoch 020, train loss=1.1828, validation accuracy=76.65%\n",
            "Epoch 021, train loss=1.1599, validation accuracy=75.11%\n",
            "Epoch 022, train loss=1.1486, validation accuracy=77.69%\n",
            "Epoch 023, train loss=1.1164, validation accuracy=79.56%\n",
            "Epoch 024, train loss=1.1144, validation accuracy=78.65%\n",
            "Epoch 025, train loss=1.0996, validation accuracy=80.23%\n",
            "Epoch 026, train loss=1.0783, validation accuracy=79.21%\n",
            "Epoch 027, train loss=1.0634, validation accuracy=76.91%\n",
            "Epoch 028, train loss=1.0431, validation accuracy=78.30%\n",
            "Epoch 029, train loss=1.0297, validation accuracy=81.41%\n",
            "Epoch 030, train loss=1.0136, validation accuracy=76.63%\n",
            "Epoch 031, train loss=1.0153, validation accuracy=82.17%\n",
            "Epoch 032, train loss=0.9965, validation accuracy=82.34%\n",
            "Epoch 033, train loss=0.9821, validation accuracy=83.02%\n",
            "Epoch 034, train loss=0.9634, validation accuracy=82.50%\n",
            "Epoch 035, train loss=0.9598, validation accuracy=83.15%\n",
            "Epoch 036, train loss=0.9473, validation accuracy=83.09%\n",
            "Epoch 037, train loss=0.9449, validation accuracy=83.09%\n",
            "Epoch 038, train loss=0.9499, validation accuracy=83.45%\n",
            "Epoch 039, train loss=0.9270, validation accuracy=83.47%\n",
            "Epoch 040, train loss=0.9205, validation accuracy=83.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation accuracy\n",
        "accuracy_val = evaluate(model, val_loader,\n",
        "                        lambda y_pred, y_batch: (y_pred.argmax(dim=1)\n",
        "                        == y_batch).float().mean(),\n",
        "                        aggregate = torch.mean)\n",
        "\n",
        "print(f'Validation Performance: {accuracy_val.item()*100:.3f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnKgxpfSs9yc",
        "outputId": "0b109060-66b3-452b-d541-680d7824589d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Performance: 83.415%\n"
          ]
        }
      ]
    }
  ]
}